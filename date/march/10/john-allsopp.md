

# Commodification

Remember not too long ago, when people cared a lot about what was under the hood of their computer? The
Manufacturer (Intel versus AMD versus Motorola), the architecture (RISC versus CISC), the size of the L1
Cache, the chip's clock speed, the bus speed, and on and on and on?

Sure, a lot of this was folks with too much time on their hands, but it was also the technology press. "Intel
Inside" was something folks cared a lot about. Chips had brand names, and billion dollar marketing budgets
(Intel after all sold not directly to the end user, but to the companies that built the system, but still
spent hundreds of millions branding and marketing Celerons and Pentiums, and Core Duos and so on).

But a funny thing happened on the way to the future. People simply stopped caring. Few people know, or could
care less about the hardware, chips, memory speed of their tablet or phone (and even their laptop). People
certainly care about the *results* of the hardware, particularly battery life and perceived performance, but
not the hardware itself, or its intrinsic characteristics.

Hardware has been almost entirely commodified.

A particularly strong indication of this is the "ARM" chip. Unlike most widely used chip architectures of the
past (exemplified by Intel's 186 instruction set), ARM chips are licensed and manufactured by a wide array of
companies, rather than purchased wholesale from a single manufacturer.

Well into 2005, with Mac OS computers all powered by PowerPC chips, debates about the real world benefits of
RISC versus CISC architectures (did you have to look the acronyms up?), and raw CPU speeds measured in clock
frequencies, versus measuring processor performance using measures such as Instructions Per Second were
"serious" topics of conversation in the tech press, and among technologists with, as I hinted, a little too
much time on their hands.

If, as little as half a decade ago you were to have suggested the almost complete commodification of the
hardware layer of computing, I doubt you would have got too many takers. 

I believe this process of commodification is a long term trend, that didn't start with PC hardware, and won't
stop there.

In the 1990s, the Internet commodified networking, once the domain of huge companies like Novell. Once upon a
time, companies paid hundreds of dollars per seat to license networking technologies for their PCs. No
longer.

I think the commodification of the operating system layer is already well under way (during a 3 or 4 year
period which has seen an explosion in new operating systems like iOS, Android, webOS, Bada, as well as the
major upgrade of Windows Phone 7, this might seem like a ludicrous thing to say, so first let me outline what
I mean by commodification).

With hardware, end user decisions were once often made based on the characteristics of CPUs, busses,
motherboards, and the like. This is essentially no longer true. Hardware performance matters, but hardware
characteristics simply don't (yes, companies do on occasion try to market their Android phones based on their
clock speed, but effectively no one cares.) When users no longer care about the characteristics of a layer of
a system, that layer has now been commodified.

So, what would it mean for the OS layer to be commodified? In it's most extreme form, end users would cease to
care about the OS as part of their platform choice, which right now would be a ludicrous assertion. At least
among affluent people in the “developed” world, a very significant percentage of users choose iPhones, in
no small part because of the unique user experience of the OS (there are of course other factors at play,
which we'll return to shortly).

But let's turn to the Android platform, the other significant smartphone platform (again, at least in the
developed world).

Unlike iOS, Android is a far more fragmented ecosystem, even before we get to the array of OS versions. The
Android user experience, unlike the rigidly controlled iOS user experience, is endlessly customized by device
manufacturers and carriers, which is the first step toward the commodification of the Android platform, which
is ironic, because these customization exist largely because of the commodification of the hardware layer.

Unlike iOS, Android form factors, screen sizes and resolutions vary significantly, from 128px x 128px swatch
like devices, to Android big screen TVs.

Amazon's recently launched Kindle Fire, based on Android, but with an entirely reworked user experience
effectively heralds the commodification of the Android OS. A vanishingly small number of people will purchase
a Kindle Fire because it is an Android based device (and few if any won't purchase one when they would
otherwise have because it __is__ an Android device).

Like power and plumbing, Android (and in time all Operating Systems) will simply become a utility - vital, but
largely unnoticed (until those times as with water and electricity that it's suddenly not available).

But I don't think the commodification of IT will end there. It will move further up the stack.

So, what sits above operating systems? Applications. Now, how in blue blazes, where the number of apps on a
platform is a key measure of the the health of that ecosystem, and where platform owners work incredibly hard
to entice developers to their platforms, could we possibly see the application layer commodified? What might
this even mean?

Why would anyone actually build an application if it were just going to be a commodity?

Well, it's already happening. In many cases, an application is simply a means of providing part of a broader
service (be that banking, or ordering a pizza for home delivery). These services, these business existed
before apps, before smart phones, before the web. Apps (like web sites, and people answering plain old
telephones) are simply a small part of the existing business.

Twitter effectively commodified Twitter client apps, by releasing their own free native clients across a wide
range of platforms, they severely undermined the market for Twitter client applications. It's Twitter the
service that matters.  

Then there are services like Netflix, where the application in many ways __is__ the service (you order, pay
for and consume the service inside the application - which of course glosses over the enormous effort behind
the scenes to make, but from the user's perspective, their engagement with the service is their engagement
with the app).

But in the case of banks, home delivery, services like NetFlix, the application is already a commodity (while
it's important that the user experience doesn't suck, that's part of the broader service design, its the
overall service, not the application itself which drives users to choose it).

Of course, this is not always true, particularly in the game category. People play Angry Birds because it is
Angry Birds. But even here, games are becoming increasingly less a stand alone product, and more part of a
broader service. Angry Birds becomes Angry Birds Rio, tied into, and enabled by not application sales, but a
broader entertainment experience.

The rise of game networks, with a continuous stream of new, engaging games is gaming as a service, which
threatens to turn gaming itself into a commodity.

So, where is the business opportunity, when everything is turning into a utility? 

Hardware, Operating Systems, applications, networks don't exist for their own sake. People use them to get
things done. They use them for the outcome, not the process.

So, focus on the outcome, what the user wants to achieve - kill time, get food, communicate, learn something -
and provide services which help them do that. You'll build sites and apps that run on browsers and operating
systems, and ultimately hardware and networks to do so, but what you're really building is the service.
Everything else is (and really always has been) but a means to that end.